\chapter{处理文本数据}
文本数据通常被表示为由字符组成的字符串。在上面给出的所有例子中，文本数据的长度
都不相同。这个特征显然与前面讨论过的数值特征有很大不同，我们需要先处理数据，然
后才能对其应用机器学习算法。
\section{用字符串表示的数据类型}
文本通常只是数据集中的字符串，但并非所有的字符串特征都应该被
当作文本来处理。
可能会遇到四种类型的字符串数据：
\begin{itemize}
    \item 分类数据
    \item 可以在语义上映射为类别的自由字符串
    \item 结构化字符串数据
    \item 文本数据
\end{itemize}

分类数据（categorical data）是来自固定列表的数据。比如你通过调查人们最喜欢的颜色
来收集数据，你向他们提供了一个下拉菜单，可以从“红色”“绿色”“蓝色”“黄色”“黑
色”“白色”“紫色”和“粉色”中选择。这样会得到一个包含 8 个不同取值的数据集，
这 8 个不同取值表示的显然是分类变量。你可以通过观察来判断你的数据是不是分类数
据（如果你看到了许多不同的字符串，那么不太可能是分类变量），并通过计算数据集中
的唯一值并绘制其出现次数的直方图来验证你的判断。

现在想象一下，你向用户提供的不是一个下拉菜单，而是一个文本框，让他们填写自己最
喜欢的颜色。许多人的回答可能是像“黑色”或“蓝色”之类的颜色名称。其他人可能会
出现笔误，使用不同的单词拼写（比如“gray”和“grey”），或使用更加形象的具体名称
（比如“午夜蓝色”）。

从
文本框中得到的回答属于上述列表中的第二类，可以在语义上映射为类别的自由字符串
（free strings that can be semantically mapped to categories）。可能最好将这种数据编码为分类
变量，你可以利用最常见的条目来选择类别，也可以自定义类别，使用户回答对应用有意
义。这样你可能会有一些标准颜色的类别，可能还有一个“多色”类别（对于像“绿色与
红色条纹”之类的回答）和“其他”类别（对于无法归类的回答）。这种字符串预处理过
程可能需要大量的人力，并且不容易自动化。如果你能够改变数据的收集方式，那么我们
强烈建议，对于分类变量能够更好表示的概念，不要使用手动输入值。

通常来说，手动输入值不与固定的类别对应，但仍有一些内在的结构（structure），比如地
址、人名或地名、日期、电话号码或其他标识符。这种类型的字符串通常难以解析，其处
理方法也强烈依赖于上下文和具体领域。

最后一类字符串数据是自由格式的文本数据（text data），由短语或句子组成。例子包括
推文、聊天记录和酒店评论，还包括莎士比亚文集、维基百科的内容或古腾堡计划收集
的 50 000 本电子书。在文本分析的语境中，数据集通
常被称为语料库（corpus）\marginpar{语料库}，每个由单个文本表示的数据点被称为文档\marginpar[文档]{文档}（document）。这
些术语来自于信息检索（information retrieval，IR）和自然语言处理（natural language
processing，NLP）的社区，它们主要针对文本数据。
\section{示例应用：电影评论的情感分析}
作为本章的一个运行示例，我们将使用由斯坦福研究员 Andrew Maas 收集的 \href{http://ai.stanford.edu/~amaas/data/sentiment/}{IMDb}
（Internet Movie Database，互联网电影数据库）网站的电影评论数据集。
\section{将文本数据表示为词袋}
用于机器学习的文本表示有一种最简单的方法，也是最有效且最常用的方法，就是使用词
袋（bag-of-words）表示。使用这种表示方式时，我们舍弃了输入文本中的大部分结构，如
章节、段落、句子和格式，只计算语料库中每个单词在每个文本中的出现频次。

对于文档语料库，计算词袋表示包括以下三个步骤：
\begin{enumerate}
    \item 分词（tokenization）。将每个文档划分为出现在其中的单词 [ 称为词例（token）]，比如
          按空格和标点划分。
    \item 构建词表（vocabulary building）。收集一个词表，里面包含出现在任意文档中的所有词，
          并对它们进行编号（比如按字母顺序排序）。
    \item 编码（encoding）。对于每个文档，计算词表中每个单词在该文档中的出现频次。
\end{enumerate}
\subsection{将词袋应用于测试数据集}
词袋表示是在 CountVectorizer 中实现的，它是一个变换器（transformer）。
\subsection{将词袋应用于电影评论}

\begin{tcolorbox}
    如果一个文档中包含训练数据中没有包含的单词，并对其调用 CountVectorizer
    的 transform 方法，那么这些单词将被忽略，因为它们没有包含在字典中。
    这对分类来说不是一个问题，因为从不在训练数据中的单词中学不到任何内
    容。但对于某些应用而言（比如垃圾邮件检测），添加一个特征来表示特定文
    档中有多少个所谓“词表外”单词可能会有所帮助。为了实现这一点，你需
    要设置 \verb|min_df|，否则这个特征在训练期间永远不会被用到。
\end{tcolorbox}
\section{停用词}
删除没有信息量的单词还有另一种方法，就是舍弃那些出现次数太多以至于没有信息量的单
词。有两种主要方法：使用特定语言的停用词（stopword）列表，或者舍弃那些出现过于频
繁的单词。scikit-learn 的 \verb|feature_extraction.text| 模块中提供了英语停用词的内置列表
\section{用tf-idf缩放数据}
另一种方法是按照我们预计的特征信息量大小来缩放特征，而不是舍弃那些认为不重要的
特征。最常见的一种做法就是使用\textbf{词频 - 逆向文档频率}（term frequency–inverse document
frequency，tf-idf）方法。\important{这一方法对在某个特定文档中经常出现的术语给予很高的权
    重，但对在语料库的许多文档中都经常出现的术语给予的权重却不高。}如果一个单词在
某个特定文档中经常出现，但在许多文档中却不常出现，那么这个单词很可能是对文
档内容的很好描述。

scikit-learn 在两个类中实现了 tf-idf 方法：TfidfTransformer 和
TfidfVectorizer，前者接受 CountVectorizer 生成的稀疏矩阵并将其变换，后者接受文本
数据并完成词袋特征提取与 tf-idf 变换。单词 $w$ 在文档 $d$ 中的 tf-idf 分数在
TfidfTransformer 类和 TfidfVectorizer 类中都有实现，其计算公式如下所示：
\begin{equation*}
    tfidf(w, d)=tf \log{\frac{N+1}{N_w+1}}+1
\end{equation*}
其中 $N$ 是训练集中的文档数量，$N_w$ 是训练集中出现单词 $w$ 的文档数量，$tf$（词频）是单词
$w$ 在查询文档 $d$（你想要变换或编码的文档）中出现的次数。两个类在计算 tf-idf 表示之后
都还应用了 L2 范数。换句话说，它们将每个文档的表示缩放到欧几里得范数为 1。利用这
种缩放方法，文档长度（单词数量）不会改变向量化表示。由于 tf-idf 实际上利用了训练数据的统计学属性，因此必须要使用Pipeline操作，避免超参调优出现高估的次优解。

请记住，tf-idf 缩放的目的是找到能够区分文档的单词，但它完全是一种无监督
技术。因此，这里的“重要”不一定与我们研究的标签相关。
\section{研究模型系数}
\section{多个单词的词袋（n元分词）}
使用词袋表示的主要缺点之一是完全舍弃了单词顺序。因此，“it’s bad, not good at all”（电
影很差，一点也不好）和“it’s good, not bad at all”（电影很好，还不错）这两个字符串的
词袋表示完全相同，尽管它们的含义相反。将“not”（不）放在单词前面，这只是上下文
很重要的一个例子（可能是一个极端的例子）。幸运的是，使用词袋表示时有一种获取上
下文的方法，就是不仅考虑单一词例的计数，而且还考虑相邻的两个或三个词例的计数。
两个词例被称为二元分词（bigram），三个词例被称为三元分词（trigram），更一般的词例
序列被称为 n 元分词（n-gram）。我们可以通过改变 CountVectorizer 或 TfidfVectorizer
的 \verb|ngram_range| 参数来改变作为特征的词例范围。\verb|ngram_range| 参数是一个元组，包含要考
虑的词例序列的最小长度和最大长度。

对于大多数应用而言，最小的词例数量应该是 1，因为单个单词通常包含丰富的含义。在
大多数情况下，添加二元分词会有所帮助。添加更长的序列（一直到五元分词）也可能有
所帮助，但这会导致特征数量的大大增加，也可能会导致过拟合，因为其中包含许多非常
具体的特征。原则上来说，二元分词的数量是一元分词数量的平方，三元分词的数量是一
元分词数量的三次方，从而导致非常大的特征空间。在实践中，更高的 n 元分词在数据中
的出现次数实际上更少，原因在于（英语）语言的结构，不过这个数字仍然很大。
\section{高级分词、词干提取与词形还原}
CountVectorizer 和 TfidfVectorizer 中的特征提取相对简单，还有更为复杂
的方法。在更加复杂的文本处理应用中，通常需要改进的步骤是词袋模型的第一步：分词
（tokenization）。这一步骤为特征提取定义了一个单词是如何构成的。

词表中通常同时包含某些单词的单数形式和复数形式，与名词的单复数形式一样，将不同的动词形式及相关单词视为不同的词
例，这不利于构建具有良好泛化性能的模型。

这个问题可以通过用\textbf{词干}（word stem）表示每个单词来解决，这一方法涉及找出或合并
（conflate）所有具有相同词干的单词。如果使用基于规则的启发法来实现（比如删除常见
的后缀），那么通常将其称为\textbf{词干提取}（stemming）。如果使用的是由已知单词形式组成的
字典（明确的且经过人工验证的系统），并且考虑了单词在句子中的作用，那么这个过程
被称为\textbf{词形还原}（lemmatization），单词的标准化形式被称为\textbf{词元}（lemma）。词干提取和
词形还原这两种处理方法都是标准化（normalization）的形式之一，标准化是指尝试提取
一个单词的某种标准形式。标准化的另一个有趣的例子是拼写校正，这种方法在实践中很
有用。

虽然 scikit-learn 没有实现这两种形式的标准化，但 CountVectorizer 允许使用 tokenizer
参数来指定使用你自己的分词器将每个文档转换为词例列表。我们可以使用 spacy 的词形
还原了创建一个可调用对象，它接受一个字符串并生成一个词元列表。

\section{主题建模与文档聚类}
常用于文本数据的一种特殊技术是主题建模（topic modeling），这是描述将每个文档分配
给一个或多个主题的任务（通常是无监督的）的概括性术语。这方面一个很好的例子是新
闻数据，它们可以被分为“政治”“体育”“金融”等主题。如果为每个文档分配一个主
题，那么这是一个文档聚类任务。如果每个文档可以有多个主题，那
么这个任务与第 3 章中的分解方法有关。我们学到的每个成分对应于一个主题，文档表示
中的成分系数告诉我们这个文档与该主题的相关性强弱。通常来说，人们在谈论主题建模
时，他们指的是一种叫作隐含狄利克雷分布（Latent Dirichlet Allocation，LDA）的特定分
解方法。
\subsection{隐含狄利克雷分布}
从直观上来看，LDA 模型试图找出频繁共同出现的单词群组（即主题）。LDA 还要求，每
个文档可以被理解为主题子集的“混合”。重要的是要理解，机器学习模型所谓的“主
题”可能不是我们通常在日常对话中所说的主题，而是更类似于 PCA 或 NMF 所提取的成分，它可能具有语义，也可能没有。即使 LDA“主题”具有语义，它可能也不是我们通常所说的主题。

对于无监督的文本文档
模型，通常最好删除非常常见的单词，否则它们可能会支配分析过程。
\section{小结}
自然语言和文本处理是一个很大的研究领域。如果你想学习更多内容，我们推荐阅读 Steven Bird、Ewan Klein 和 Edward
Loper 合著的 \href{http://shop.oreilly.com/product/9780596516499.do}{Natural Language Processing with Python} 一书，其中给出了 NLP 的概述，并介绍了 nltk 这个用
于 NLP 的 Python 库。另一本很好且概念性更强的书是 Christopher Manning、Prabhakar
Raghavan 和 Hinrich Schuütze 合著的标准参考，\href{http://nlp.stanford.edu/IR-book/}{Introduction to Information Retrieval}，其中介绍了信息检索、NLP 和机器学习中的基本算法。这两本
书都有可以免费访问的在线版本。如前所述，CountVectorizer 类和 TfidfVectorizer 类仅
实现了相对简单的文本处理方法。对于更高级的文本处理方法，我们推荐使用 Python 包
spacy（一个相对较新的包，但非常高效，且设计良好）、nltk（一个非常完善且完整的库，
但有些过时）和 gensim（着重于主题建模的 NLP 包）。