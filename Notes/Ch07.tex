\chapter{处理文本数据}
文本数据通常被表示为由字符组成的字符串。在上面给出的所有例子中，文本数据的长度
都不相同。这个特征显然与前面讨论过的数值特征有很大不同，我们需要先处理数据，然
后才能对其应用机器学习算法。
\section{用字符串表示的数据类型}
文本通常只是数据集中的字符串，但并非所有的字符串特征都应该被
当作文本来处理。
可能会遇到四种类型的字符串数据：
\begin{itemize}
    \item 分类数据
    \item 可以在语义上映射为类别的自由字符串
    \item 结构化字符串数据
    \item 文本数据
\end{itemize}

分类数据（categorical data）是来自固定列表的数据。比如你通过调查人们最喜欢的颜色
来收集数据，你向他们提供了一个下拉菜单，可以从“红色”“绿色”“蓝色”“黄色”“黑
色”“白色”“紫色”和“粉色”中选择。这样会得到一个包含 8 个不同取值的数据集，
这 8 个不同取值表示的显然是分类变量。你可以通过观察来判断你的数据是不是分类数
据（如果你看到了许多不同的字符串，那么不太可能是分类变量），并通过计算数据集中
的唯一值并绘制其出现次数的直方图来验证你的判断。

现在想象一下，你向用户提供的不是一个下拉菜单，而是一个文本框，让他们填写自己最
喜欢的颜色。许多人的回答可能是像“黑色”或“蓝色”之类的颜色名称。其他人可能会
出现笔误，使用不同的单词拼写（比如“gray”和“grey”），或使用更加形象的具体名称
（比如“午夜蓝色”）。

从
文本框中得到的回答属于上述列表中的第二类，可以在语义上映射为类别的自由字符串
（free strings that can be semantically mapped to categories）。可能最好将这种数据编码为分类
变量，你可以利用最常见的条目来选择类别，也可以自定义类别，使用户回答对应用有意
义。这样你可能会有一些标准颜色的类别，可能还有一个“多色”类别（对于像“绿色与
红色条纹”之类的回答）和“其他”类别（对于无法归类的回答）。这种字符串预处理过
程可能需要大量的人力，并且不容易自动化。如果你能够改变数据的收集方式，那么我们
强烈建议，对于分类变量能够更好表示的概念，不要使用手动输入值。

通常来说，手动输入值不与固定的类别对应，但仍有一些内在的结构（structure），比如地
址、人名或地名、日期、电话号码或其他标识符。这种类型的字符串通常难以解析，其处
理方法也强烈依赖于上下文和具体领域。

最后一类字符串数据是自由格式的文本数据（text data），由短语或句子组成。例子包括
推文、聊天记录和酒店评论，还包括莎士比亚文集、维基百科的内容或古腾堡计划收集
的 50 000 本电子书。在文本分析的语境中，数据集通
常被称为语料库（corpus）\marginpar{语料库}，每个由单个文本表示的数据点被称为文档\marginpar[文档]{文档}（document）。这
些术语来自于信息检索（information retrieval，IR）和自然语言处理（natural language
processing，NLP）的社区，它们主要针对文本数据。
\section{示例应用：电影评论的情感分析}
作为本章的一个运行示例，我们将使用由斯坦福研究员 Andrew Maas 收集的 \href{http://ai.stanford.edu/~amaas/data/sentiment/}{IMDb}
（Internet Movie Database，互联网电影数据库）网站的电影评论数据集。
\section{将文本数据表示为词袋}
用于机器学习的文本表示有一种最简单的方法，也是最有效且最常用的方法，就是使用词
袋（bag-of-words）表示。使用这种表示方式时，我们舍弃了输入文本中的大部分结构，如
章节、段落、句子和格式，只计算语料库中每个单词在每个文本中的出现频次。

对于文档语料库，计算词袋表示包括以下三个步骤：
\begin{enumerate}
    \item 分词（tokenization）。将每个文档划分为出现在其中的单词 [ 称为词例（token）]，比如
          按空格和标点划分。
    \item 构建词表（vocabulary building）。收集一个词表，里面包含出现在任意文档中的所有词，
          并对它们进行编号（比如按字母顺序排序）。
    \item 编码（encoding）。对于每个文档，计算词表中每个单词在该文档中的出现频次。
\end{enumerate}
\subsection{将词袋应用于测试数据集}
词袋表示是在 CountVectorizer 中实现的，它是一个变换器（transformer）。
\subsection{将词袋应用于电影评论}

\begin{tcolorbox}
    如果一个文档中包含训练数据中没有包含的单词，并对其调用 CountVectorizer
    的 transform 方法，那么这些单词将被忽略，因为它们没有包含在字典中。
    这对分类来说不是一个问题，因为从不在训练数据中的单词中学不到任何内
    容。但对于某些应用而言（比如垃圾邮件检测），添加一个特征来表示特定文
    档中有多少个所谓“词表外”单词可能会有所帮助。为了实现这一点，你需
    要设置 \verb|min_df|，否则这个特征在训练期间永远不会被用到。
\end{tcolorbox}
\section{停用词}
删除没有信息量的单词还有另一种方法，就是舍弃那些出现次数太多以至于没有信息量的单
词。有两种主要方法：使用特定语言的停用词（stopword）列表，或者舍弃那些出现过于频
繁的单词。scikit-learn 的 feature_extraction.text 模块中提供了英语停用词的内置列表
\section{用tf-idf缩放数据}
\section{研究模型系数}
\section{多个单词的词袋（n元分词）}
\section{高级分词、词干提取与词形还原}
\section{主题建模与文档聚类}
\subsection{隐含狄利克雷分布}